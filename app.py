import streamlit as st
import fitz  # PyMuPDF
from openai import OpenAI
from datetime import datetime
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import A4
import io
import tiktoken

# Configuraci√≥n API moderna
client = OpenAI(api_key=st.secrets["openai_api_key"])

MAX_CARACTERES_POR_PDF = 70000
MAX_OUTPUT_TOKENS = 6000
MAX_INPUT_TOKENS = 120000

if "analisis_clinicos" not in st.session_state:
    st.session_state["analisis_clinicos"] = {}

if "historial_respuestas" not in st.session_state:
    st.session_state["historial_respuestas"] = []

def contar_tokens(texto, modelo="gpt-4-turbo"):
    enc = tiktoken.encoding_for_model(modelo)
    return len(enc.encode(texto))

def extract_text_from_pdf(uploaded_file):
    with fitz.open(stream=uploaded_file.read(), filetype="pdf") as doc:
        text = ""
        for page in doc:
            raw = page.get_text("text")
            clean = raw.replace('\x00', ' ').replace('\u2028', ' ')
            clean = clean.encode("utf-8", "ignore").decode("utf-8", "ignore")
            clean = ' '.join(clean.split())
            text += clean + "\n\n"
    return text

def generar_analisis_clinico(texto_total, seccion_objetivo):
    if seccion_objetivo == "Todo el art√≠culo":
        objetivo_prompt = "analiza el art√≠culo completo"
    else:
        objetivo_prompt = f"analiza exclusivamente la secci√≥n de {seccion_objetivo.lower()}"

    prompt = f"""
Act√∫a como m√©dico especialista en medicina materno-fetal.

Tienes a continuaci√≥n el contenido de un art√≠culo cient√≠fico extra√≠do de un PDF:

{texto_total}

Por favor, {objetivo_prompt} y genera un informe profesional para revisi√≥n por especialistas cl√≠nicos. El informe debe estar estructurado, enfocado en evidencia m√©dica clara, y ser √∫til para discusi√≥n acad√©mica o aplicaci√≥n cl√≠nica.
"""

    if contar_tokens(prompt) > MAX_INPUT_TOKENS:
        st.error("‚ö†Ô∏è El contenido del art√≠culo es demasiado extenso para el modelo. Intenta reducir su tama√±o.")
        return "ERROR: Texto muy largo para el modelo."

    respuesta = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=MAX_OUTPUT_TOKENS
    )
    return respuesta.choices[0].message.content

def generar_pdf(nombre_archivo, contenido, seccion):
    buffer = io.BytesIO()
    c = canvas.Canvas(buffer, pagesize=A4)
    width, height = A4
    y = height - 80

    c.setFont("Helvetica-Bold", 14)
    c.drawString(50, y, "An√°lisis de Literatura M√©dica FLASOG 2025")
    y -= 25

    c.setFont("Helvetica", 12)
    c.drawString(50, y, f"T√≠tulo del art√≠culo: {nombre_archivo}")
    y -= 20
    c.drawString(50, y, f"Fecha de an√°lisis: {datetime.today().strftime('%Y-%m-%d')}")
    y -= 20
    c.drawString(50, y, f"Secci√≥n analizada: {seccion}")
    y -= 30

    c.setFont("Helvetica", 10)
    for linea in contenido.split('\n'):

        for fragmento in [linea[i:i+100] for i in range(0, len(linea), 100)]:
            if y < 40:
                c.showPage()
                y = height - 60
                c.setFont("Helvetica", 10)
            c.drawString(50, y, fragmento)
            y -= 14

    c.save()
    buffer.seek(0)
    return buffer

st.set_page_config(page_title="FLASOG 2025 - An√°lisis de Literatura M√©dica", layout="wide")
st.title("üìò An√°lisis de Literatura M√©dica FLASOG 2025")
st.markdown("### Suba uno o m√°s art√≠culos PDF para generar informes cl√≠nicos independientes")

uploaded_files = st.file_uploader("üìÑ Sube tus archivos PDF", type="pdf", accept_multiple_files=True)
seccion_objetivo = st.radio("¬øQu√© secci√≥n deseas analizar?",
                            ["Todo el art√≠culo", "Metodolog√≠a", "Resultados", "Conclusiones"], index=0)

if uploaded_files:
    for archivo in uploaded_files:
        nombre = archivo.name
        texto = extract_text_from_pdf(archivo)
        st.markdown(f"---
### üìÑ Informe para: `{nombre}`")
        st.info(f"üìè Caracteres extra√≠dos: {len(texto)}")

        if len(texto) > MAX_CARACTERES_POR_PDF:
            st.warning("‚ö†Ô∏è Recortando texto a 70,000 caracteres.")
            texto = texto[:MAX_CARACTERES_POR_PDF]

        if st.button(f"üß† Analizar `{nombre}`"):
            with st.spinner("Generando informe cl√≠nico..."):
                resultado = generar_analisis_clinico(texto, seccion_objetivo)
                st.session_state["analisis_clinicos"][nombre] = resultado

        if nombre in st.session_state["analisis_clinicos"]:
            st.subheader("üìù Informe cl√≠nico generado:")
            st.write(st.session_state["analisis_clinicos"][nombre])

            pdf_bytes = generar_pdf(nombre, st.session_state["analisis_clinicos"][nombre], seccion_objetivo)
            st.download_button("üìÑ Descargar informe en PDF", pdf_bytes, file_name=f"{nombre}_informe.pdf")

st.markdown("---")
st.subheader("üí¨ Preguntas cl√≠nicas personalizadas")

pregunta = st.text_input("Haz una pregunta sobre los art√≠culos analizados:")

if st.button("‚ùì Responder con IA"):
    if pregunta.strip():
        contexto = "

contexto = "\n\n".join(st.session_state["analisis_clinicos"].values())
        with st.spinner("Buscando respuesta..."):
            prompt = f"""Act√∫a como m√©dico materno-fetal. Usa el siguiente contexto cl√≠nico para responder:

{contexto}

PREGUNTA: {pregunta}
"""
            respuesta = client.chat.completions.create(
                model="gpt-4-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1500
            ).choices[0].message.content
            st.session_state["historial_respuestas"].append((pregunta, respuesta))
    else:
        st.warning("Escribe una pregunta v√°lida.")

if st.session_state["historial_respuestas"]:
    st.subheader("üìö Historial de preguntas y respuestas")
    for i, (q, r) in enumerate(st.session_state["historial_respuestas"]):
        st.markdown(f"**{i+1}. Pregunta:** {q}")
        st.markdown(f"üß† {r}")
